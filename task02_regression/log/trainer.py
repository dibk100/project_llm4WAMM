from transformers import Trainer
import torch
import torch.nn.functional as F
from transformers import TrainerCallback
import wandb
import os

class CustomTrainer(Trainer):
    def __init__(self, *args, binary_bool=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.binary_bool = binary_bool

    def _save_checkpoint(self, model, trial, metrics=None):
        epoch = int(self.state.epoch or 0)
        checkpoint_folder = f"checkpoint-epoch-{epoch}"
        output_dir = os.path.join(self.args.output_dir, checkpoint_folder)
        os.makedirs(output_dir, exist_ok=True)

        self.save_model(output_dir=output_dir)
        self._save_optimizer_and_scheduler(output_dir)
        self._save_rng_state(output_dir)

        if metrics is not None:
            print("metricsê°€ ì—†ëŠ” ê²½ìš°ê°€ ìˆì–´?")
            self.log_metrics("eval", metrics)
            self.save_metrics("eval", metrics)

        if self.args.should_save:
            self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))

    def save_model(self, output_dir=None, _internal_call=False):                        # output_dir ì´ê±° ë•Œë¬¸ì´ì—ˆìŒ...
        print("ğŸ’¾ğŸ’¾ [CustomTrainer.save_model()] í˜¸ì¶œë¨!")  # ë°˜ë“œì‹œ ë³´ì—¬ì•¼ í•¨
        
        epoch = int(self.state.epoch or 0)
        output_dir = os.path.join(self.args.output_dir, f"checkpoint-epoch-{epoch}")
        os.makedirs(output_dir, exist_ok=True)

        print(f"ğŸ’¾ğŸ’¾ ìˆ˜ë™ ì €ì¥ í˜¸ì¶œ - ëª¨ë¸ ì €ì¥: {output_dir}")
        torch.save(self.model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
        # super().save_model(output_dir)                                                # ìˆ˜ë™ ì €ì¥ ì•ˆë˜ì„œ ì •ë§ ì‚¬ëŒ í˜ë“¤ê²Œ í•œë‹¤.. 

        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(output_dir)
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))

class PrintMetricsCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None:
            print("\n***** Val Results *****")
            for key, value in metrics.items():
                print(f"  {key} = {value:.4f}")
        return control

class WandbEpochCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        # state.epochì€ floatì´ì§€ë§Œ ì†Œìˆ˜ì  ì´í•˜ ë²„ë¦¬ê³  int ì²˜ë¦¬ ê°€ëŠ¥
        epoch = int(state.epoch)
        metrics = kwargs.get("metrics")
        if metrics:
            wandb.log(metrics, step=epoch)