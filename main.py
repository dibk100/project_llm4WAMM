import yaml
import torch
from transformers import BertTokenizer
from model import BertForMultiLabelClassification  
from dataset import MultiLabelDatasetProcessor     
from train import train
from eval import evaluate
from utils import save_best_model
import os
import wandb
from torch.utils.data import DataLoader
from torch.utils.data import random_split

# ìë™ ë¡œê·¸ì¸
os.environ["WANDB_API_KEY"] = "your-wandb-api-key"  # ì—¬ê¸°ì— ë³¸ì¸ì˜ API í‚¤ ì…ë ¥
wandb.login()

def main():
    model_name = "base.yaml"                    # ëª¨ë¸ ë³€ê²½ì‹œ í™œìš©
    with open(model_name) as f:
        config = yaml.safe_load(f)

    project = config["project"]
    wandb.init(project=project, config=config)
    config = wandb.config  
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = BertTokenizer.from_pretrained(config["model"]["name"])
    
    train_processor = MultiLabelDatasetProcessor(
        json_path=config["data"]["train_file"],
        label_list=config["model"]["labels"],   # ë¬¸ìì—´ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        tokenizer_name=config["model"]["name"],
        max_length=config["training"]["max_length"]
    )

    train_dataset = train_processor.get_dataset()
    
    torch.manual_seed(42)

    total_len = len(train_dataset)
    val_len = int(total_len * 0.2) # 20%ë¥¼ validationìœ¼ë¡œ ì‚¬ìš©
    train_len = total_len - val_len

    train_subset, val_subset = random_split(train_dataset, [train_len, val_len])
         
    train_loader = DataLoader(train_subset, batch_size=config["training"]["batch_size"], shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=config["training"]["batch_size"])

    model = BertForMultiLabelClassification(
        model_name=config["model"]["name"],
        num_labels=len(config["model"]["labels"])
    ).to(device)

    num_epochs = config["training"]["epochs"]
    patience = config["training"]["patience"]
    num_labels = len(config["model"]["labels"])
    
    # Loss í•¨ìˆ˜:
    # criterion = torch.nn.BCEWithLogitsLoss()
    # criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=config["training"]["lr"])

    # ì´ˆê¸°í™”
    best_f1 = 0.0
    epochs_no_improve = 0
    
    for epoch in range(num_epochs):
        train_avg_loss, train_f1 = train(model, train_loader, optimizer, device,num_epochs)
        test_f1  = evaluate(model, val_loader, device)
        
        ## ê¸°ë¡
        wandb.log({
        "train/loss": train_avg_loss,
        "train/f1": train_f1,
        "test/f1": test_f1,
        "epoch": epoch + 1
        })

        ## ê¸°ë¡
        print(f"Epoch {epoch+1}/{num_epochs} | "
            f"Train Loss: {train_avg_loss:.4f}, Train F1: {train_f1:.4f}, "
            f"Test F1: {test_f1 :.4f}")
        
        if test_f1 > best_f1:
            best_f1 = test_f1
            epochs_no_improve = 0
            save_best_model(model, save_dir="outputs", base_name=model_name, num_label=labels, best_f1=best_f1)
        
        else :
            epochs_no_improve += 0
            print(f"âš ï¸ No improvement for {epochs_no_improve} epoch(s)")
            
            if epochs_no_improve >= patience:
                print("â›” Early stopping triggered.")
                break

    print(f"ğŸ‰ Training complete. Best Val F1: {best_f1:.4f}")

if __name__ == "__main__":
    main()